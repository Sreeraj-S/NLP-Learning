{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "632ce9de-99d9-42c1-8f53-0a2b49beee2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/names/Russian.txt', 'data/names/Korean.txt', 'data/names/Portuguese.txt', 'data/names/German.txt', 'data/names/Polish.txt', 'data/names/Greek.txt', 'data/names/Spanish.txt', 'data/names/Scottish.txt', 'data/names/Chinese.txt', 'data/names/Japanese.txt', 'data/names/French.txt', 'data/names/Italian.txt', 'data/names/English.txt', 'data/names/Dutch.txt', 'data/names/Irish.txt', 'data/names/Czech.txt', 'data/names/Arabic.txt', 'data/names/Vietnamese.txt']\n",
      "Slusarski\n",
      "['Russian', 'Korean', 'Portuguese', 'German', 'Polish', 'Greek', 'Spanish', 'Scottish', 'Chinese', 'Japanese', 'French', 'Italian', 'English', 'Dutch', 'Irish', 'Czech', 'Arabic', 'Vietnamese']\n"
     ]
    }
   ],
   "source": [
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "print(findFiles('data/names/*.txt'))\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "print(unicodeToAscii('Ślusàrski'))\n",
    "\n",
    "# Build the category_lines dictionary, a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "for filename in findFiles('data/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_lines[category] = lines\n",
    "print(all_categories)\n",
    "n_categories = len(all_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e91d813-90b8-47c8-8f7b-b04d79efd922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abandonato', 'Abatangelo', 'Abatantuono', 'Abate', 'Abategiovanni']\n"
     ]
    }
   ],
   "source": [
    "print(category_lines['Italian'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a19e750f-6e48-44af-be3c-b6563c9f70d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "torch.Size([1, 57])\n",
      "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "print(letterToTensor('J').size())\n",
    "\n",
    "print(lineToTensor('Jones'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b96ff37f-f123-422d-a54d-e5dd7f1bdfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.h2o(hidden)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "n_hidden = 128\n",
    "rnn = RNN(n_letters, n_hidden, n_categories).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29973b78-96df-4bb7-ae01-8de4dd3bb208",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = letterToTensor('A').to(device)\n",
    "hidden = torch.zeros(1, n_hidden).to(device)\n",
    "\n",
    "output, next_hidden = rnn(input, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f641fe13-1b1e-4d83-ae9d-c0c9e49be2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.9281, -2.9461, -2.8480, -2.9062, -2.9650, -2.8874, -2.9429, -2.8749,\n",
      "         -2.8617, -2.9507, -2.8660, -2.9888, -2.8131, -2.7531, -2.8646, -2.8520,\n",
      "         -2.8631, -2.9455]], device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input = lineToTensor('Albert').to(device)\n",
    "hidden = torch.zeros(1, n_hidden).to(device)\n",
    "\n",
    "output, next_hidden = rnn(input[0], hidden)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abe77b5c-9066-4d2c-af49-a59000bcb11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Dutch', 13)\n"
     ]
    }
   ],
   "source": [
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return all_categories[category_i], category_i\n",
    "\n",
    "print(categoryFromOutput(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7cb9d3c-16c7-4d20-b7bd-025337544343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category = Korean / line = Yeon\n",
      "category = Scottish / line = King\n",
      "category = Arabic / line = Shamoon\n",
      "category = Czech / line = Miksatkova\n",
      "category = Vietnamese / line = Han\n",
      "category = Irish / line = Flynn\n",
      "category = German / line = Paternoster\n",
      "category = Arabic / line = Essa\n",
      "category = Japanese / line = Shigi\n",
      "category = Japanese / line = Sugase\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "def randomTrainingExample():\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "    line_tensor = lineToTensor(line)\n",
    "    return category, line, category_tensor, line_tensor\n",
    "\n",
    "for i in range(10):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    print('category =', category, '/ line =', line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "787db94c-742a-484c-a627-f5d88358cf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a950d5ab-fa23-4ec1-aa7b-c8016f6518b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
    "\n",
    "def train(category_tensor, line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn.forward(line_tensor[i].to(device), hidden.to(device))\n",
    "\n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "\n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0c9130c-d792-411b-842e-92477cf6f877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 0% (0m 1s) 2.9300 Eliopoulos / Dutch ✗ (Greek)\n",
      "200 0% (0m 2s) 2.8633 Cardozo / Dutch ✗ (Portuguese)\n",
      "300 0% (0m 2s) 2.9056 Fonseca / Dutch ✗ (Portuguese)\n",
      "400 0% (0m 3s) 2.8792 Demakis / Dutch ✗ (Greek)\n",
      "500 0% (0m 4s) 2.7678 Clarkson / Dutch ✗ (English)\n",
      "600 0% (0m 4s) 2.8894 Abreu / Dutch ✗ (Spanish)\n",
      "700 0% (0m 5s) 2.6853 Kloeten / Dutch ✓\n",
      "800 0% (0m 5s) 2.8186 Anetakis / Dutch ✗ (Greek)\n",
      "900 0% (0m 6s) 2.8682 Hoang / Dutch ✗ (Vietnamese)\n",
      "1000 1% (0m 7s) 2.9146 Aloia / Russian ✗ (Italian)\n",
      "1100 1% (0m 7s) 2.8441 Gou / Dutch ✗ (Chinese)\n",
      "1200 1% (0m 8s) 2.8359 Zha / Dutch ✗ (Chinese)\n",
      "1300 1% (0m 8s) 2.9908 Murchadh / Dutch ✗ (Irish)\n",
      "1400 1% (0m 9s) 2.8956 Wasem / Dutch ✗ (Arabic)\n",
      "1500 1% (0m 10s) 2.9290 Koo / Portuguese ✗ (Korean)\n",
      "1600 1% (0m 10s) 2.8801 Ghanem / Polish ✗ (Arabic)\n",
      "1700 1% (0m 11s) 2.8369 Gomatos / Dutch ✗ (Greek)\n",
      "1800 1% (0m 11s) 2.8103 Madeira / Japanese ✗ (Portuguese)\n",
      "1900 1% (0m 11s) 2.8846 O'Toole / Dutch ✗ (Irish)\n",
      "2000 2% (0m 12s) 2.8233 Deguchi / Polish ✗ (Japanese)\n",
      "2100 2% (0m 12s) 2.8431 Noh / Arabic ✗ (Korean)\n",
      "2200 2% (0m 12s) 2.7886 Slusarczyk / Dutch ✗ (Polish)\n",
      "2300 2% (0m 13s) 2.7924 Gabler / Dutch ✗ (German)\n",
      "2400 2% (0m 13s) 2.6989 Snider / Dutch ✓\n",
      "2500 2% (0m 13s) 2.8478 Naifeh / Polish ✗ (Arabic)\n",
      "2600 2% (0m 14s) 2.9569 Belmonte / Dutch ✗ (Spanish)\n",
      "2700 2% (0m 14s) 2.7574 Hayden / Dutch ✗ (Irish)\n",
      "2800 2% (0m 14s) 2.6070 Milionis / Greek ✓\n",
      "2900 2% (0m 15s) 2.9438 Gavril / Arabic ✗ (Greek)\n",
      "3000 3% (0m 15s) 2.8312 Neil / Arabic ✗ (Irish)\n",
      "3100 3% (0m 15s) 2.6901 Rahal / Arabic ✓\n",
      "3200 3% (0m 16s) 2.9961 Lilwall / Portuguese ✗ (English)\n",
      "3300 3% (0m 16s) 2.6504 Cino / Portuguese ✗ (Italian)\n",
      "3400 3% (0m 16s) 2.8277 Robertson / Greek ✗ (Scottish)\n",
      "3500 3% (0m 17s) 2.5755 Watoga / Japanese ✓\n",
      "3600 3% (0m 17s) 2.7631 Rypka / Polish ✗ (Czech)\n",
      "3700 3% (0m 18s) 2.8350 Alphen / Irish ✗ (Dutch)\n",
      "3800 3% (0m 18s) 2.8355 Vandroogenbroeck / Polish ✗ (Dutch)\n",
      "3900 3% (0m 18s) 2.5727 Rizzo / Italian ✓\n",
      "4000 4% (0m 19s) 2.3896 Houlis / Greek ✓\n",
      "4100 4% (0m 19s) 2.9225 Peary / Arabic ✗ (Czech)\n",
      "4200 4% (0m 19s) 2.4137 Urogataya / Japanese ✓\n",
      "4300 4% (0m 20s) 2.2613 Dubicki / Polish ✓\n",
      "4400 4% (0m 20s) 2.5677 Simoes / Greek ✗ (Portuguese)\n",
      "4500 4% (0m 20s) 2.6350 Arthur / Arabic ✗ (French)\n",
      "4600 4% (0m 21s) 2.9218 Vo / Portuguese ✗ (Vietnamese)\n",
      "4700 4% (0m 21s) 1.9113 Demarchis / Greek ✓\n",
      "4800 4% (0m 22s) 1.7897 Kaminski / Polish ✓\n",
      "4900 4% (0m 22s) 2.6618 Renaud / Irish ✗ (French)\n",
      "5000 5% (0m 22s) 3.1999 Kokkali / Polish ✗ (Greek)\n",
      "5100 5% (0m 23s) 1.6461 Anetakis / Greek ✓\n",
      "5200 5% (0m 23s) 2.7090 Nader / German ✗ (Arabic)\n",
      "5300 5% (0m 23s) 2.5176 Vuong / Vietnamese ✓\n",
      "5400 5% (0m 24s) 2.4931 Yan / Irish ✗ (Chinese)\n",
      "5500 5% (0m 24s) 2.1217 Narita / Japanese ✓\n",
      "5600 5% (0m 24s) 3.2890 Rendon / Irish ✗ (Spanish)\n",
      "5700 5% (0m 25s) 2.6537 O'Neal / Greek ✗ (Irish)\n",
      "5800 5% (0m 25s) 2.2913 Homutov / Russian ✓\n",
      "5900 5% (0m 25s) 2.1370 Bellandi / Italian ✓\n",
      "6000 6% (0m 26s) 2.1437 Hirayama / Spanish ✗ (Japanese)\n",
      "6100 6% (0m 26s) 3.4544 Bishara / Italian ✗ (Arabic)\n",
      "6200 6% (0m 26s) 1.7595 Sokolsky / Polish ✓\n",
      "6300 6% (0m 27s) 1.6003 Mosconi / Italian ✓\n",
      "6400 6% (0m 27s) 0.3578 Chrysanthopoulos / Greek ✓\n",
      "6500 6% (0m 27s) 1.7718 Namiki / Japanese ✓\n",
      "6600 6% (0m 28s) 1.7989 An / Chinese ✗ (Vietnamese)\n",
      "6700 6% (0m 28s) 1.9959 Staska / Polish ✗ (Czech)\n",
      "6800 6% (0m 28s) 1.8472 Kulhanek / Czech ✓\n",
      "6900 6% (0m 29s) 3.3771 Lobo / Vietnamese ✗ (Spanish)\n",
      "7000 7% (0m 29s) 2.5302 Denend / German ✗ (Dutch)\n",
      "7100 7% (0m 29s) 2.1497 Favreau / German ✗ (French)\n",
      "7200 7% (0m 30s) 2.2682 Herback / French ✗ (Czech)\n",
      "7300 7% (0m 30s) 0.9043 Yoshida / Japanese ✓\n",
      "7400 7% (0m 30s) 2.2935 Cathain / Arabic ✗ (Irish)\n",
      "7500 7% (0m 31s) 2.4853 Lattimore / French ✗ (English)\n",
      "7600 7% (0m 31s) 1.6801 Takudome / Japanese ✓\n",
      "7700 7% (0m 32s) 2.0395 Sargent / German ✗ (French)\n",
      "7800 7% (0m 32s) 2.8690 Caro / Japanese ✗ (Spanish)\n",
      "7900 7% (0m 32s) 1.3563 Sander / German ✓\n",
      "8000 8% (0m 33s) 3.7661 Komon / Arabic ✗ (Japanese)\n",
      "8100 8% (0m 33s) 3.2387 Fernandez / German ✗ (Spanish)\n",
      "8200 8% (0m 33s) 2.6173 Rome / Arabic ✗ (French)\n",
      "8300 8% (0m 34s) 0.6159 Shadid / Arabic ✓\n",
      "8400 8% (0m 34s) 2.9272 Friedrich / Russian ✗ (Czech)\n",
      "8500 8% (0m 34s) 1.2073 Hang / Chinese ✓\n",
      "8600 8% (0m 35s) 1.5927 Achthoven / Dutch ✓\n",
      "8700 8% (0m 35s) 2.1886 Schindler / Dutch ✗ (German)\n",
      "8800 8% (0m 35s) 2.1467 Abreu / German ✗ (Portuguese)\n",
      "8900 8% (0m 36s) 2.8789 Segal / Scottish ✗ (French)\n",
      "9000 9% (0m 36s) 1.3148 Nahas / Arabic ✓\n",
      "9100 9% (0m 36s) 2.7745 Morcos / Greek ✗ (Arabic)\n",
      "9200 9% (0m 36s) 1.5117 Emelyantsev / Russian ✓\n",
      "9300 9% (0m 37s) 1.4354 Shin / Chinese ✗ (Korean)\n",
      "9400 9% (0m 37s) 2.1714 Delgado / Japanese ✗ (Portuguese)\n",
      "9500 9% (0m 38s) 1.8123 Kotoku / Japanese ✓\n",
      "9600 9% (0m 38s) 2.0029 Shirdov / Scottish ✗ (Russian)\n",
      "9700 9% (0m 38s) 2.7608 Parmar / Arabic ✗ (English)\n",
      "9800 9% (0m 39s) 1.5039 Ta / Chinese ✗ (Vietnamese)\n",
      "9900 9% (0m 39s) 0.7262 Yun / Chinese ✓\n",
      "10000 10% (0m 39s) 0.5120 Janowski / Polish ✓\n",
      "10100 10% (0m 40s) 0.8292 Morello / Italian ✓\n",
      "10200 10% (0m 40s) 1.2346 Rosario / Portuguese ✓\n",
      "10300 10% (0m 40s) 1.8020 Marov / Russian ✓\n",
      "10400 10% (0m 41s) 2.1633 Sarkozy / Irish ✗ (French)\n",
      "10500 10% (0m 41s) 1.4423 Simpson / Scottish ✓\n",
      "10600 10% (0m 41s) 2.5005 Bell / German ✗ (Scottish)\n",
      "10700 10% (0m 42s) 2.8131 Oliver / German ✗ (French)\n",
      "10800 10% (0m 42s) 0.1789 Manoukarakis / Greek ✓\n",
      "10900 10% (0m 42s) 0.8831 Rang / Chinese ✓\n",
      "11000 11% (0m 43s) 2.1915 Wallace / French ✗ (Scottish)\n",
      "11100 11% (0m 43s) 1.4871 Mackenzie / Russian ✗ (Scottish)\n",
      "11200 11% (0m 44s) 2.5544 Kopp / Arabic ✗ (German)\n",
      "11300 11% (0m 44s) 2.4019 Prchal / Scottish ✗ (Czech)\n",
      "11400 11% (0m 44s) 2.6058 Alst / French ✗ (Dutch)\n",
      "11500 11% (0m 45s) 2.5688 Crump / Arabic ✗ (English)\n",
      "11600 11% (0m 45s) 1.1209 Xue / Chinese ✓\n",
      "11700 11% (0m 45s) 0.7451 Paszek / Polish ✓\n",
      "11800 11% (0m 46s) 2.5593 Strand / Vietnamese ✗ (German)\n",
      "11900 11% (0m 46s) 3.3001 Costello / Italian ✗ (English)\n",
      "12000 12% (0m 46s) 2.8632 Matheson / Russian ✗ (English)\n",
      "12100 12% (0m 47s) 3.2621 Miller / Dutch ✗ (Scottish)\n",
      "12200 12% (0m 47s) 0.9443 Nardi / Italian ✓\n",
      "12300 12% (0m 47s) 2.0514 Mateus / Greek ✗ (Portuguese)\n",
      "12400 12% (0m 48s) 2.2813 Descoteaux / English ✗ (French)\n",
      "12500 12% (0m 48s) 2.3080 Gray / Arabic ✗ (Scottish)\n",
      "12600 12% (0m 49s) 2.2382 Szweda / Spanish ✗ (Polish)\n",
      "12700 12% (0m 49s) 2.8184 Abdulkhabiroff / English ✗ (Russian)\n",
      "12800 12% (0m 49s) 0.4746 Martinelli / Italian ✓\n",
      "12900 12% (0m 50s) 2.6258 Hill / Irish ✗ (Scottish)\n",
      "13000 13% (0m 50s) 0.8134 Sitko / Polish ✓\n",
      "13100 13% (0m 51s) 3.9371 Park  / Polish ✗ (Korean)\n",
      "13200 13% (0m 51s) 0.8981 Xun / Chinese ✓\n",
      "13300 13% (0m 51s) 0.9417 Antipas / Greek ✓\n",
      "13400 13% (0m 52s) 1.4020 Cathan / Irish ✓\n",
      "13500 13% (0m 52s) 0.7273 Belesis / Greek ✓\n",
      "13600 13% (0m 53s) 0.7718 Quan / Chinese ✓\n",
      "13700 13% (0m 53s) 1.8785 Dupont / German ✗ (French)\n",
      "13800 13% (0m 54s) 1.2701 Erepov / Russian ✓\n",
      "13900 13% (0m 54s) 1.8650 Coupe / French ✓\n",
      "14000 14% (0m 54s) 2.3607 Devin / Scottish ✗ (Irish)\n",
      "14100 14% (0m 55s) 1.1312 Dao / Vietnamese ✓\n",
      "14200 14% (0m 55s) 0.2434 Kozlowski / Polish ✓\n",
      "14300 14% (0m 55s) 1.2003 Vesnovsky / Polish ✗ (Russian)\n",
      "14400 14% (0m 56s) 4.1548 Laar / Arabic ✗ (Dutch)\n",
      "14500 14% (0m 56s) 3.1303 Karlovsky / Russian ✗ (Czech)\n",
      "14600 14% (0m 57s) 3.3039 Hughes / Dutch ✗ (Scottish)\n",
      "14700 14% (0m 57s) 2.9254 Schlantz / Spanish ✗ (Czech)\n",
      "14800 14% (0m 57s) 3.3968 Papageorge / Irish ✗ (Greek)\n",
      "14900 14% (0m 58s) 0.9616 Ferreiro / Portuguese ✓\n",
      "15000 15% (0m 58s) 1.7674 Haik / Polish ✗ (Arabic)\n",
      "15100 15% (0m 58s) 0.7637 Rocchi / Italian ✓\n",
      "15200 15% (0m 59s) 1.8744 Lavoie / French ✓\n",
      "15300 15% (0m 59s) 0.9109 Kassis / Greek ✗ (Arabic)\n",
      "15400 15% (0m 59s) 2.1789 Eggby / Czech ✗ (English)\n",
      "15500 15% (0m 59s) 2.1634 Jesson / Scottish ✗ (English)\n",
      "15600 15% (1m 0s) 0.3539 Ardovini / Italian ✓\n",
      "15700 15% (1m 0s) 0.6456 Kim / Korean ✓\n",
      "15800 15% (1m 1s) 0.4920 Eatros / Greek ✓\n",
      "15900 15% (1m 1s) 3.0661 Acqua / Arabic ✗ (Italian)\n",
      "16000 16% (1m 1s) 3.2023 Cham / Chinese ✗ (Arabic)\n",
      "16100 16% (1m 2s) 1.9772 Victor / Scottish ✗ (French)\n",
      "16200 16% (1m 2s) 1.1880 Kieu / Vietnamese ✓\n",
      "16300 16% (1m 2s) 2.5651 Farrer / German ✗ (English)\n",
      "16400 16% (1m 3s) 1.7316 Turrell / Dutch ✗ (English)\n",
      "16500 16% (1m 3s) 1.1316 Brauer / German ✓\n",
      "16600 16% (1m 3s) 1.9229 Page / French ✓\n",
      "16700 16% (1m 4s) 1.7275 Han / Chinese ✗ (Korean)\n",
      "16800 16% (1m 4s) 0.4244 Zenilov / Russian ✓\n",
      "16900 16% (1m 4s) 0.2097 Malinowski / Polish ✓\n",
      "17000 17% (1m 5s) 1.5081 Hajicek / Polish ✗ (Czech)\n",
      "17100 17% (1m 5s) 0.7702 Gaertner / German ✓\n",
      "17200 17% (1m 5s) 0.8295 Rademakers / Dutch ✓\n",
      "17300 17% (1m 6s) 1.3302 Lieberenz / German ✓\n",
      "17400 17% (1m 6s) 1.4452 Gu / Vietnamese ✗ (Korean)\n",
      "17500 17% (1m 7s) 0.8866 Asghar / Arabic ✓\n",
      "17600 17% (1m 7s) 2.9317 Costa / Czech ✗ (Portuguese)\n",
      "17700 17% (1m 7s) 0.9413 Maehata / Japanese ✓\n",
      "17800 17% (1m 7s) 1.9634 Paquet / Arabic ✗ (French)\n",
      "17900 17% (1m 8s) 2.3851 Miller / German ✗ (Scottish)\n",
      "18000 18% (1m 8s) 2.3598 Felix / French ✗ (Spanish)\n",
      "18100 18% (1m 9s) 3.6083 Guirguis / Greek ✗ (Arabic)\n",
      "18200 18% (1m 9s) 2.3565 Winograd / Arabic ✗ (Polish)\n",
      "18300 18% (1m 9s) 0.4349 Yoo / Korean ✓\n",
      "18400 18% (1m 10s) 1.6140 Campo / Portuguese ✗ (Spanish)\n",
      "18500 18% (1m 10s) 1.9886 Lobo / Italian ✗ (Spanish)\n",
      "18600 18% (1m 10s) 3.2350 Maly / Irish ✗ (Czech)\n",
      "18700 18% (1m 11s) 4.4415 Gajos / Greek ✗ (Polish)\n",
      "18800 18% (1m 11s) 1.4751 Kurmochi / Italian ✗ (Japanese)\n",
      "18900 18% (1m 11s) 3.2564 Bolivar / French ✗ (Spanish)\n",
      "19000 19% (1m 12s) 1.9691 Cham / Vietnamese ✗ (Arabic)\n",
      "19100 19% (1m 12s) 1.0788 Pasternack / Czech ✗ (Polish)\n",
      "19200 19% (1m 12s) 2.2831 Vivas / Portuguese ✗ (Spanish)\n",
      "19300 19% (1m 13s) 1.7769 Cann / Dutch ✓\n",
      "19400 19% (1m 13s) 0.1883 Altoviti / Italian ✓\n",
      "19500 19% (1m 13s) 0.9373 Bai / Chinese ✓\n",
      "19600 19% (1m 14s) 0.1484 Aldebrandi / Italian ✓\n",
      "19700 19% (1m 14s) 2.0826 Rana / Japanese ✗ (Spanish)\n",
      "19800 19% (1m 14s) 1.3914 Hong / Chinese ✗ (Korean)\n",
      "19900 19% (1m 15s) 1.3255 Donoghue / Irish ✓\n",
      "20000 20% (1m 15s) 0.9068 Bieber / German ✓\n",
      "20100 20% (1m 15s) 2.2868 Pae / Chinese ✗ (Korean)\n",
      "20200 20% (1m 16s) 0.3470 Zdunowski / Polish ✓\n",
      "20300 20% (1m 16s) 1.0530 Daher / Arabic ✓\n",
      "20400 20% (1m 16s) 1.3520 Campbell / Scottish ✓\n",
      "20500 20% (1m 17s) 1.2428 Shi / Korean ✗ (Chinese)\n",
      "20600 20% (1m 17s) 1.2123 Shi / Korean ✗ (Chinese)\n",
      "20700 20% (1m 17s) 2.2852 Tailler / German ✗ (French)\n",
      "20800 20% (1m 18s) 1.7429 Tuma / Japanese ✗ (Arabic)\n",
      "20900 20% (1m 18s) 1.4009 Sala / Spanish ✓\n",
      "21000 21% (1m 18s) 1.3669 Banh / Vietnamese ✓\n",
      "21100 21% (1m 19s) 1.8733 Travers / Dutch ✗ (French)\n",
      "21200 21% (1m 19s) 2.0330 Moreno / Italian ✗ (Portuguese)\n",
      "21300 21% (1m 19s) 1.4837 Abdrazakoff / Russian ✓\n",
      "21400 21% (1m 20s) 3.2180 Hakimi / Japanese ✗ (Arabic)\n",
      "21500 21% (1m 20s) 0.9258 Colman / Irish ✓\n",
      "21600 21% (1m 20s) 0.8119 Kwong / Korean ✗ (Chinese)\n",
      "21700 21% (1m 21s) 0.9981 Lau / Vietnamese ✗ (Chinese)\n",
      "21800 21% (1m 21s) 2.9480 D'cruze / French ✗ (Portuguese)\n",
      "21900 21% (1m 21s) 1.1894 De santigo / Portuguese ✓\n",
      "22000 22% (1m 22s) 3.0556 Aalst / French ✗ (Dutch)\n",
      "22100 22% (1m 22s) 0.2526 Abankin / Russian ✓\n",
      "22200 22% (1m 23s) 2.3257 Rousses / Spanish ✗ (Greek)\n",
      "22300 22% (1m 23s) 3.3855 Zuniga / Japanese ✗ (Spanish)\n",
      "22400 22% (1m 23s) 0.4443 Quang / Vietnamese ✓\n",
      "22500 22% (1m 24s) 1.8570 Tadhgan / Arabic ✗ (Irish)\n",
      "22600 22% (1m 24s) 3.2755 Prchal / Irish ✗ (Czech)\n",
      "22700 22% (1m 24s) 2.2024 Felix / Russian ✗ (French)\n",
      "22800 22% (1m 25s) 0.7853 La / Vietnamese ✓\n",
      "22900 22% (1m 25s) 3.0159 Foster / German ✗ (English)\n",
      "23000 23% (1m 25s) 3.6437 Gerges / Portuguese ✗ (Arabic)\n",
      "23100 23% (1m 26s) 1.6467 Liang / Vietnamese ✗ (Chinese)\n",
      "23200 23% (1m 26s) 1.2384 Mckenzie / Scottish ✓\n",
      "23300 23% (1m 26s) 1.0303 Gallego / Spanish ✓\n",
      "23400 23% (1m 26s) 4.4916 Jones / Dutch ✗ (Scottish)\n",
      "23500 23% (1m 27s) 0.0431 Sardelis / Greek ✓\n",
      "23600 23% (1m 27s) 1.4661 Kattan / Arabic ✓\n",
      "23700 23% (1m 27s) 3.4154 Buckholtz / Japanese ✗ (German)\n",
      "23800 23% (1m 28s) 0.3336 Yakuta / Japanese ✓\n",
      "23900 23% (1m 28s) 5.0186 Juhno / Vietnamese ✗ (Russian)\n",
      "24000 24% (1m 29s) 0.8257 Man / Chinese ✓\n",
      "24100 24% (1m 29s) 1.4826 Simoes / Arabic ✗ (Portuguese)\n",
      "24200 24% (1m 29s) 0.6688 Romijnders / Dutch ✓\n",
      "24300 24% (1m 30s) 3.2609 Brabbery / English ✗ (Czech)\n",
      "24400 24% (1m 30s) 1.0040 Kramer / German ✓\n",
      "24500 24% (1m 30s) 1.5635 Shannon / Irish ✓\n",
      "24600 24% (1m 31s) 1.0556 Kollen / Dutch ✓\n",
      "24700 24% (1m 31s) 1.2033 Savona / Italian ✓\n",
      "24800 24% (1m 31s) 2.5535 Baz / Chinese ✗ (Arabic)\n",
      "24900 24% (1m 32s) 2.1421 Sinagra / Arabic ✗ (Italian)\n",
      "25000 25% (1m 32s) 5.7622 Roosa / Italian ✗ (Dutch)\n",
      "25100 25% (1m 32s) 2.3471 Moreno / Italian ✗ (Portuguese)\n",
      "25200 25% (1m 33s) 1.4009 Antwerpen / Dutch ✓\n",
      "25300 25% (1m 33s) 1.6083 Medeiros / Greek ✗ (Portuguese)\n",
      "25400 25% (1m 33s) 0.8930 Szwedko / Polish ✓\n",
      "25500 25% (1m 34s) 1.8080 Shalhoub / Vietnamese ✗ (Arabic)\n",
      "25600 25% (1m 34s) 1.1396 Mach / Vietnamese ✓\n",
      "25700 25% (1m 34s) 0.1212 Niemczyk / Polish ✓\n",
      "25800 25% (1m 35s) 0.6850 Takemago / Japanese ✓\n",
      "25900 25% (1m 35s) 0.5653 Ferreiro / Portuguese ✓\n",
      "26000 26% (1m 35s) 1.8643 Bran / Vietnamese ✗ (Irish)\n",
      "26100 26% (1m 36s) 1.0021 Araujo / Portuguese ✓\n",
      "26200 26% (1m 36s) 2.7955 Roux / Chinese ✗ (French)\n",
      "26300 26% (1m 36s) 0.0558 Sfakianos / Greek ✓\n",
      "26400 26% (1m 37s) 3.1822 Krantz / Spanish ✗ (Dutch)\n",
      "26500 26% (1m 37s) 0.0983 Angelopoulos / Greek ✓\n",
      "26600 26% (1m 37s) 0.2412 Filipowski / Polish ✓\n",
      "26700 26% (1m 38s) 1.7497 Voclain / Irish ✗ (French)\n",
      "26800 26% (1m 38s) 1.2365 Rademaker / German ✓\n",
      "26900 26% (1m 38s) 0.6825 O'Ryan / Irish ✓\n",
      "27000 27% (1m 39s) 2.5203 Wakelin / Russian ✗ (English)\n",
      "27100 27% (1m 39s) 1.3985 O'Mooney / Irish ✓\n",
      "27200 27% (1m 40s) 2.5595 Tron / Korean ✗ (Vietnamese)\n",
      "27300 27% (1m 40s) 1.0365 Hong / Korean ✗ (Chinese)\n",
      "27400 27% (1m 40s) 2.6344 Miller / German ✗ (Scottish)\n",
      "27500 27% (1m 41s) 1.7108 Maclean / Irish ✗ (Scottish)\n",
      "27600 27% (1m 41s) 2.2567 Enomoto / Italian ✗ (Japanese)\n",
      "27700 27% (1m 41s) 3.1362 Mcmanus / Portuguese ✗ (English)\n",
      "27800 27% (1m 41s) 2.5453 Zeman / Irish ✗ (Czech)\n",
      "27900 27% (1m 42s) 1.4096 Perrault / French ✓\n",
      "28000 28% (1m 42s) 1.2813 Balakhovski / Japanese ✗ (Russian)\n",
      "28100 28% (1m 42s) 1.3386 Tatsuno / Portuguese ✗ (Japanese)\n",
      "28200 28% (1m 43s) 1.2414 Saller / German ✓\n",
      "28300 28% (1m 43s) 2.4389 Hachirobei / Polish ✗ (Japanese)\n",
      "28400 28% (1m 43s) 1.2873 Mai / Chinese ✗ (Vietnamese)\n",
      "28500 28% (1m 44s) 2.6946 Venne / French ✗ (Dutch)\n",
      "28600 28% (1m 44s) 1.1942 Segers / Dutch ✓\n",
      "28700 28% (1m 45s) 1.7469 Sai / Chinese ✗ (Vietnamese)\n",
      "28800 28% (1m 45s) 1.8934 Reid / Arabic ✗ (Scottish)\n",
      "28900 28% (1m 45s) 0.6236 Thao / Vietnamese ✓\n",
      "29000 28% (1m 45s) 5.2199 Macshuibhne / Japanese ✗ (Irish)\n",
      "29100 29% (1m 46s) 1.0962 O'Connell / Irish ✓\n",
      "29200 29% (1m 46s) 2.0746 Keenan / Scottish ✗ (English)\n",
      "29300 29% (1m 46s) 2.6828 Lines / Portuguese ✗ (English)\n",
      "29400 29% (1m 47s) 0.9711 Gagnier / French ✓\n",
      "29500 29% (1m 47s) 0.2967 Ryoo / Korean ✓\n",
      "29600 29% (1m 47s) 2.7296 Spicer / German ✗ (English)\n",
      "29700 29% (1m 48s) 0.7487 Cai / Chinese ✓\n",
      "29800 29% (1m 48s) 1.5347 Pho / Korean ✗ (Vietnamese)\n",
      "29900 29% (1m 48s) 0.8097 Esteves / Portuguese ✓\n",
      "30000 30% (1m 49s) 0.9571 Mustafa / Arabic ✓\n",
      "30100 30% (1m 49s) 0.2107 Biancardi / Italian ✓\n",
      "30200 30% (1m 49s) 0.4077 Delgado / Portuguese ✓\n",
      "30300 30% (1m 50s) 1.7662 Langlais / Portuguese ✗ (French)\n",
      "30400 30% (1m 50s) 1.4049 Ott / German ✓\n",
      "30500 30% (1m 50s) 0.1212 Takahama / Japanese ✓\n",
      "30600 30% (1m 51s) 2.4706 D'cruze / French ✗ (Spanish)\n",
      "30700 30% (1m 51s) 1.1602 Yeo / Vietnamese ✗ (Korean)\n",
      "30800 30% (1m 52s) 2.9284 Fraser / German ✗ (Scottish)\n",
      "30900 30% (1m 52s) 2.8400 Adam / Arabic ✗ (Irish)\n",
      "31000 31% (1m 52s) 1.2352 Labriola / Spanish ✗ (Italian)\n",
      "31100 31% (1m 53s) 1.3834 Campos / Greek ✗ (Portuguese)\n",
      "31200 31% (1m 53s) 0.3979 Tadhgan / Irish ✓\n",
      "31300 31% (1m 53s) 0.6102 Rozinek / Czech ✓\n",
      "31400 31% (1m 54s) 0.9671 Basurto / Spanish ✗ (Portuguese)\n",
      "31500 31% (1m 54s) 2.2555 Lennon / Scottish ✗ (Irish)\n",
      "31600 31% (1m 54s) 0.1680 Tsumemasa / Japanese ✓\n",
      "31700 31% (1m 55s) 0.7119 Yong / Chinese ✓\n",
      "31800 31% (1m 55s) 1.0268 Suh / Vietnamese ✗ (Korean)\n",
      "31900 31% (1m 55s) 1.2006 Shan / Chinese ✓\n",
      "32000 32% (1m 56s) 0.7741 Ashida / Japanese ✓\n",
      "32100 32% (1m 56s) 2.6328 Abramenkoff / Czech ✗ (Russian)\n",
      "32200 32% (1m 56s) 2.1037 Gaber / German ✗ (Arabic)\n",
      "32300 32% (1m 57s) 0.6906 Shamon / Arabic ✓\n",
      "32400 32% (1m 57s) 1.2052 Rier / German ✓\n",
      "32500 32% (1m 57s) 0.6339 Whyte / Scottish ✓\n",
      "32600 32% (1m 58s) 1.1553 Cao / Chinese ✗ (Vietnamese)\n",
      "32700 32% (1m 58s) 1.6116 Flett / Scottish ✗ (English)\n",
      "32800 32% (1m 59s) 0.7443 Vela / Spanish ✓\n",
      "32900 32% (1m 59s) 1.1806 Sum / Korean ✗ (Chinese)\n",
      "33000 33% (1m 59s) 0.9625 Egger / German ✓\n",
      "33100 33% (2m 0s) 0.5999 Bouloukos / Greek ✓\n",
      "33200 33% (2m 0s) 0.7484 Timonin / Russian ✓\n",
      "33300 33% (2m 0s) 3.2142 Haanrath / Scottish ✗ (Dutch)\n",
      "33400 33% (2m 1s) 1.2045 Von wegberg / Dutch ✗ (German)\n",
      "33500 33% (2m 1s) 2.1608 Toma / Japanese ✗ (Arabic)\n",
      "33600 33% (2m 1s) 0.9310 Stroggylis / Greek ✓\n",
      "33700 33% (2m 2s) 1.1528 Quraishi / Japanese ✗ (Arabic)\n",
      "33800 33% (2m 2s) 0.0500 Miyagi / Japanese ✓\n",
      "33900 33% (2m 3s) 0.7724 Vo / Vietnamese ✓\n",
      "34000 34% (2m 3s) 0.3637 Usiskin / Russian ✓\n",
      "34100 34% (2m 3s) 1.0671 Huynh / Vietnamese ✓\n",
      "34200 34% (2m 4s) 1.4118 Tojo / Vietnamese ✗ (Japanese)\n",
      "34300 34% (2m 4s) 0.4798 Ramires / Portuguese ✓\n",
      "34400 34% (2m 5s) 1.1849 Craig / Scottish ✓\n",
      "34500 34% (2m 5s) 0.7506 Sherak / Czech ✓\n",
      "34600 34% (2m 5s) 0.0097 Katzukov / Russian ✓\n",
      "34700 34% (2m 6s) 1.7898 Froy / Scottish ✗ (English)\n",
      "34800 34% (2m 6s) 2.2345 Buggenum / Russian ✗ (Dutch)\n",
      "34900 34% (2m 6s) 2.4564 Sankovsky / Russian ✗ (Czech)\n",
      "35000 35% (2m 7s) 0.7464 Yim / Korean ✓\n",
      "35100 35% (2m 7s) 2.0311 Sandoval / Czech ✗ (Spanish)\n",
      "35200 35% (2m 7s) 3.1404 Szwarc / German ✗ (Polish)\n",
      "35300 35% (2m 8s) 0.9409 Chweh / Vietnamese ✗ (Korean)\n",
      "35400 35% (2m 8s) 2.5533 Kaspar / French ✗ (German)\n",
      "35500 35% (2m 8s) 3.4894 Michel / Irish ✗ (French)\n",
      "35600 35% (2m 9s) 3.4112 Salazar / French ✗ (Portuguese)\n",
      "35700 35% (2m 9s) 0.8750 Gerber / German ✓\n",
      "35800 35% (2m 9s) 0.3083 Slaski / Polish ✓\n",
      "35900 35% (2m 10s) 0.2573 Voltolini / Italian ✓\n",
      "36000 36% (2m 10s) 2.6940 Matocha / Japanese ✗ (Czech)\n",
      "36100 36% (2m 11s) 5.8979 Olguin / Irish ✗ (Spanish)\n",
      "36200 36% (2m 11s) 0.3893 Ding / Chinese ✓\n",
      "36300 36% (2m 11s) 2.4963 Stotzky / Russian ✗ (Czech)\n",
      "36400 36% (2m 12s) 0.0278 Antonopoulos / Greek ✓\n",
      "36500 36% (2m 12s) 3.2290 Ross / Greek ✗ (Scottish)\n",
      "36600 36% (2m 12s) 1.0269 Tivoli / Italian ✓\n",
      "36700 36% (2m 13s) 1.9117 Araullo / Italian ✗ (Portuguese)\n",
      "36800 36% (2m 13s) 1.9770 Heinrich / French ✗ (German)\n",
      "36900 36% (2m 13s) 0.7822 Kruger / German ✓\n",
      "37000 37% (2m 13s) 0.6381 Mcintyre / Scottish ✓\n",
      "37100 37% (2m 14s) 0.2609 Slusarski / Polish ✓\n",
      "37200 37% (2m 14s) 2.2911 Waite / French ✗ (English)\n",
      "37300 37% (2m 14s) 2.5447 Kalb / Scottish ✗ (Arabic)\n",
      "37400 37% (2m 15s) 1.5757 Antoun / Arabic ✓\n",
      "37500 37% (2m 15s) 0.0729 Papadopulos / Greek ✓\n",
      "37600 37% (2m 15s) 0.2572 Sarraf / Arabic ✓\n",
      "37700 37% (2m 16s) 1.3723 Fearnley / French ✗ (English)\n",
      "37800 37% (2m 16s) 0.5515 Naifeh / Arabic ✓\n",
      "37900 37% (2m 16s) 4.0496 Gotti / Italian ✗ (German)\n",
      "38000 38% (2m 17s) 0.2567 Naomhan / Irish ✓\n",
      "38100 38% (2m 17s) 1.3880 Sinagra / Czech ✗ (Italian)\n",
      "38200 38% (2m 17s) 0.5718 Graner / German ✓\n",
      "38300 38% (2m 18s) 3.1813 Kurogane / Irish ✗ (Japanese)\n",
      "38400 38% (2m 18s) 0.4610 Santiago / Portuguese ✓\n",
      "38500 38% (2m 18s) 4.0075 Munro / Italian ✗ (Scottish)\n",
      "38600 38% (2m 19s) 3.4738 Michel / Irish ✗ (Polish)\n",
      "38700 38% (2m 19s) 0.6766 Dong / Chinese ✓\n",
      "38800 38% (2m 20s) 0.5946 Dai / Chinese ✓\n",
      "38900 38% (2m 20s) 0.2501 Khoury / Arabic ✓\n",
      "39000 39% (2m 20s) 1.3103 Duong / Vietnamese ✓\n",
      "39100 39% (2m 21s) 5.3697 Oshin / Russian ✗ (Japanese)\n",
      "39200 39% (2m 21s) 1.4874 Sierra / Portuguese ✗ (Spanish)\n",
      "39300 39% (2m 21s) 0.5415 O'Loughlin / Irish ✓\n",
      "39400 39% (2m 22s) 0.3758 Thai / Vietnamese ✓\n",
      "39500 39% (2m 22s) 2.3419 Neville / French ✗ (Irish)\n",
      "39600 39% (2m 22s) 0.6613 Shin / Korean ✓\n",
      "39700 39% (2m 23s) 0.0168 Albanesi / Italian ✓\n",
      "39800 39% (2m 23s) 3.5328 Breda / Spanish ✗ (Dutch)\n",
      "39900 39% (2m 23s) 1.9313 Giunta / Japanese ✗ (Italian)\n",
      "40000 40% (2m 24s) 0.5253 Amari / Arabic ✓\n",
      "40100 40% (2m 24s) 2.2130 Klemper / German ✗ (Czech)\n",
      "40200 40% (2m 24s) 1.1165 Crespo / Portuguese ✗ (Spanish)\n",
      "40300 40% (2m 25s) 0.4039 Kanavos / Greek ✓\n",
      "40400 40% (2m 25s) 1.3519 Lieberenz / German ✓\n",
      "40500 40% (2m 26s) 0.0473 Protopsaltis / Greek ✓\n",
      "40600 40% (2m 26s) 1.7859 De ath / English ✓\n",
      "40700 40% (2m 26s) 0.2583 Agostini / Italian ✓\n",
      "40800 40% (2m 27s) 1.1098 Hoshino / Japanese ✓\n",
      "40900 40% (2m 27s) 5.4328 Albert / Dutch ✗ (Spanish)\n",
      "41000 41% (2m 27s) 1.3749 Marquerink / Czech ✗ (German)\n",
      "41100 41% (2m 28s) 0.7599 Cho / Korean ✓\n",
      "41200 41% (2m 28s) 1.7808 Rios / Greek ✗ (Portuguese)\n",
      "41300 41% (2m 29s) 1.2985 Iturburua / Spanish ✓\n",
      "41400 41% (2m 29s) 2.6263 Munro / Italian ✗ (Scottish)\n",
      "41500 41% (2m 29s) 0.1973 Airaldi / Italian ✓\n",
      "41600 41% (2m 30s) 0.3484 Bandoni / Italian ✓\n",
      "41700 41% (2m 30s) 0.5603 Dang / Vietnamese ✓\n",
      "41800 41% (2m 31s) 0.2279 Gorskin / Russian ✓\n",
      "41900 41% (2m 31s) 1.4719 Ferreiro / Italian ✗ (Portuguese)\n",
      "42000 42% (2m 31s) 4.2143 To The First Page / Japanese ✗ (Russian)\n",
      "42100 42% (2m 32s) 1.1066 Macleod / French ✗ (Scottish)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, n_iters \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     24\u001b[0m     category, line, category_tensor, line_tensor \u001b[38;5;241m=\u001b[39m randomTrainingExample()\n\u001b[0;32m---> 25\u001b[0m     output, loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcategory_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mline_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     current_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# Print ``iter`` number, loss, name and guess\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(category_tensor, line_tensor)\u001b[0m\n\u001b[1;32m      9\u001b[0m     output, hidden \u001b[38;5;241m=\u001b[39m rnn\u001b[38;5;241m.\u001b[39mforward(line_tensor[i]\u001b[38;5;241m.\u001b[39mto(device), hidden\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, category_tensor)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Add parameters' gradients to their values, multiplied by learning rate\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m rnn\u001b[38;5;241m.\u001b[39mparameters():\n",
      "File \u001b[0;32m~/miniconda3/envs/MLtorch/lib/python3.8/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/MLtorch/lib/python3.8/site-packages/torch/autograd/__init__.py:204\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "n_iters = 100000\n",
    "print_every = 100\n",
    "plot_every = 1000\n",
    "\n",
    "\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output, loss = train(category_tensor.to(device), line_tensor.to(device))\n",
    "    current_loss += loss\n",
    "\n",
    "    # Print ``iter`` number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        guess, guess_i = categoryFromOutput(output.to(device))\n",
    "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa977861-a8fb-4476-9986-7e65c11c718f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674a3f12-a177-42be-a9ff-26d3de8de486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of correct guesses in a confusion matrix\n",
    "confusion = torch.zeros(n_categories, n_categories)\n",
    "n_confusion = 10000\n",
    "\n",
    "# Just return an output given a line\n",
    "def evaluate(line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i].to(device), hidden.to(device))\n",
    "\n",
    "    return output\n",
    "\n",
    "# Go through a bunch of examples and record which are correctly guessed\n",
    "for i in range(n_confusion):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output = evaluate(line_tensor)\n",
    "    guess, guess_i = categoryFromOutput(output)\n",
    "    category_i = all_categories.index(category)\n",
    "    confusion[category_i][guess_i] += 1\n",
    "\n",
    "# Normalize by dividing every row by its sum\n",
    "for i in range(n_categories):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()\n",
    "\n",
    "# Set up plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion.numpy())\n",
    "fig.colorbar(cax)\n",
    "\n",
    "# Set up axes\n",
    "ax.set_xticklabels([''] + all_categories, rotation=90)\n",
    "ax.set_yticklabels([''] + all_categories)\n",
    "\n",
    "# Force label at every tick\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "# sphinx_gallery_thumbnail_number = 2\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6098ae92-546f-457c-8851-c24877dc746b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_line, n_predictions=3):\n",
    "    print('\\n> %s' % input_line)\n",
    "    with torch.no_grad():\n",
    "        output = evaluate(lineToTensor(input_line))\n",
    "\n",
    "        # Get top N categories\n",
    "        topv, topi = output.topk(n_predictions, 1, True)\n",
    "        predictions = []\n",
    "\n",
    "        for i in range(n_predictions):\n",
    "            value = topv[0][i].item()\n",
    "            category_index = topi[0][i].item()\n",
    "            print('(%.2f) %s' % (value, all_categories[category_index]))\n",
    "            predictions.append([value, all_categories[category_index]])\n",
    "\n",
    "predict('Dovesky')\n",
    "predict('Jackson')\n",
    "predict('Satoshi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2db7b66-edd9-4822-9acd-00abf387df00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
